* Aggie Expert Harvest

  This image builds and/or serves IAM data in a fuseki endpoint.

** Environmental Variables

   In most cases, this image needs to be run authorized as a google service
   account.  These credentials are passed as an enviromental variable

   ~GOOGLE_APPLICATION_CREDENTIALS_JSON~ This is a locally defined variable,
   that is used to activate the gcloud service account, explicitly as ~gcloud
   auth activate-service-account --key-file=- <<<
   "${GOOGLE_APPLICATION_CREDENTIALS_JSON}"~.  There is no default value


** Methods of running

*** Building the data

    When building a new set of IAM data, the image will fetch data from UCD IAM
    services. The data is converted into the VIVO schema.  Physically, the data
    can be formatted as either ~jsonld~ or as a ~hdt~ file.  This can only be
    successful when the container is running in the UCD VPN, and will not work
    in the cloud.

    In addition, the built data can be pushed to the ~$GCS_GRAPH_STORAGE~ cloud
    bucket.

    A typical invocation might be as shown below, where the VOLUME used to
    create the data is ephemeral with data creation.

    #+begin_src bash
      docker run --name iam --env GOOGLE_APPLICATION_CREDENTIALS_JSON="$(gcloud secrets versions access latest --secret='aggie-expert-data-harvester-key' | jq -c .)"\
      --interactive --tty --rm  gcr.io/digital-ucdavis-edu/aggie-experts/harvest HDT=build iam.hdt hdt-push
    #+end_src

    You might instead want to view the data first, before running.  In that
    case, you might first, create the data, and then start the server.

    #+begin_src bash
      docker run --name iam -p 3030:3030 --env GOOGLE_APPLICATION_CREDENTIALS_JSON="$(gcloud secrets versions access latest --secret='aggie-expert-data-harvester-key' | jq -c .)"\
      --interactive --tty --rm gcr.io/digital-ucdavis-edu/aggie-experts/harvest HDT=build iam.hdt server
    #+end_src

    If you are satisfied, you can then push the data to the cloud:

    #+begin_src bash
    docker exec iam  --env GOOGLE_APPLICATION_CREDENTIALS_JSON="$(gcloud secrets versions access latest --secret='aggie-expert-data-harvester-key' | jq -c .)"\
    /harvest-entrypoint.sh hdt-push
    #+end_src

**** iam.hdt
     Retrieving IAM data requires access to the IAM server. There is an google
     cloud secret ~projects/326679616213/secrets/ucdid_auth~ that needs to be
     accessable from the service account.


*** Serving the data

    You can also use this image to serve existing data.  In this case, the
    container first fetches the data from GCS_GRAPH_STORAGE and the starts the
    server. This uses the ~HDT=import~ context, which is the default.  This can
    be run in the cloud or locally.  An example startup for that would be:

    #+begin_src bash
      docker run --name iam -p 3030:3030 --env GOOGLE_APPLICATION_CREDENTIALS_JSON="$(gcloud secrets versions access latest --secret='aggie-expert-data-harvester-key' | jq -c .)"\
      --interactive --tty --rm gcr.io/digital-ucdavis-edu/aggie-experts/harvest:v1.0.0 server
    #+end_src

    Alternatively, for testing or for examining the most basic shared VIVO data,
    you can use the ~HDT=stubs~ context.  This will create nearly empty ~iam~
    and ~experts~ graphs. In that case, you don't need an credentials at all.

    #+begin_src bash
      docker run --name iam -p 3030:3030 \
      --interactive --tty --rm gcr.io/digital-ucdavis-edu/aggie-experts/harvest:v1.0.0 HDT=stubs server
    #+end_src


*** Other examples

    You can bind mount to the ~/home/ucd.process~ VOLUME if you'd like a local
    version of the data. You can also ask for jsonld versions of the build
    files. This example saves all the intermediate data you might use for
    testing.

    #+begin_src bash
      docker run --name=iam --interactive --tty --rm \
      --mount type=bind,source=/home/quinn/aggie-test,target=/home/ucd.process \
      gcr.io/digital-ucdavis-edu/aggie-experts/harvest:v1.0.0 HDT=build iam.json
   #+end_src

   You can also use the ~bash~ command to skip building or serving, and just
   investigate the container

       #+begin_src bash
      docker run --name=iam --interactive --tty --rm \
      --mount type=bind,source=/home/quinn/aggie-test,target=/home/ucd.process \
      gcr.io/digital-ucdavis-edu/aggie-experts/harvest:v1.0.0 bash
   #+end_src


*** docker-compose.yaml

    This example shows two examples of using the container as a fuseki endpoint
    server. ~iam~ service imports the data, and the harvester doesn't.  Both
    endpoints are exposed.

    These examples use a .env file that holds the service account credentials,
    created with:

    #+begin_src bash
      echo GOOGLE_APPLICATION_CREDENTIALS_JSON="$(gcloud secrets versions access latest --secret='aggie-expert-data-harvester-key' | jq -c .)" > .env
    #+end_src

    #+begin_src yaml :tangle docker-compose.yaml
version: '3'

services:
  iam:
    image: gcr.io/digital-ucdavis-edu/aggie-experts/harvest:v1.0.0
    environment:
      - GOOGLE_APPLICATION_CREDENTIALS_JSON=${GOOGLE_APPLICATION_CREDENTIALS_JSON}
    volumes:
      - iam-data:/home/ucd.process
    ports:
      - ${IAM_HOST_PORT:-3030}:3030
    command: hdt-import server

  harvester:
    image: gcr.io/digital-ucdavis-edu/aggie-experts/harvest:v1.0.0
    environment:
      - GOOGLE_APPLICATION_CREDENTIALS_JSON=${GOOGLE_APPLICATION_CREDENTIALS_JSON}
    volumes:
      - harvester-data:/home/ucd.process
    ports:
      - ${HARVESTER_HOST_PORT:-3031}:3030
    command: stubs server

volumes:
  iam-data:
  harvester-data:
   #+end_src

**** Using the fuseki service
     :PROPERTIES:
     :header-args:sparql: :url http://localhost:3031/experts/sparql
     :END:

     When running, this compose file can be used to show how the service
     endpoint can be used.  For example, this query (running on the harvester)
     is requesting all data from the ~http://iam.ucdavis.edu/~ graph running on
     the IAM server.

     #+begin_src sparql :format raw :wrap SRC ttl
       PREFIX experts: <http://experts.ucdavis.edu/>
       PREFIX str: <http://nlp2rdf.lod2.eu/schema/string/>
       PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
       PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
       PREFIX ucdlib: <http://schema.library.ucdavis.edu/schema#>
       CONSTRUCT {
         ?s ?p ?o.
       }
       #select *
       WHERE {
         bind(uri(concat(str(experts:),'expert/',MD5('quinn@ucdavis.edu'))) as ?user)
         SERVICE <http://iam:3030/experts/query> {
           graph <http://iam.ucdavis.edu/> {
             ?user a ucdlib:Expert .
             ?s ?p ?o;
                .
             filter(regex(str(?s),concat('^',str(?user),'#?')))
           }
         }
       }
     #+end_src


** More info
  This is a fuseki/jena system that can be used to create the file based inputs
  for cloud computing environment.  Multiple versions can be started up at any
  given time, and used to populate the cloud based services.  They are not
  dependant on a centralized fuseki database, although they do have some shared
  read-only databases in the form of HDT files.  These can be installed locally,
  or used as a service in the cloud.

  The image uses the standard [[https://jena.apache.org/documentation/fuseki2/fuseki-layout.html][fuseki layout]], FUSEKI_HOME and FUSEKI_BASE.  This
  allows us to have the configuration files in the image.  The databases are not
  in their default location, but are instead at:
  /home/ucd.process/fuseki/databases.  The fuseki layout file identifies that we
  need remove the ${FUSEKI_BASE}/system and ${FUSEKI_BASE}/system_files to reset
  the server.  By default, the system runs fuseki as a normal user without root
  elevation.  This is typically a volume mount that is ephemeral for the
  container's lifespan, but it can be bound to more persistent mounts.

*** Data Management TBD

    Here are some potential data management considerations.

**** hdt vs tdb2

     Although hidden, the current build process actually creates TDB2 files as
     well as hdt.  It's possible that this could be a better methodology for
     serving the data. In particular, we can add statistics to the TDB files.

    The TDB [[https://jena.apache.org/documentation/tdb/optimizer.html][Optimizer]] gives information on creating the statistics to create
    better optimization strategies.

**** hdt vs json

     Another alternative is to simply save the data as ~jsonld~ files, and have
     the import step use ~http~ to post this data into a running server.  One
     advantage of this is the you could have these data added as seperate greaphs
     to a TDB file that would be used for the harvesting.  Query optimiation is
     better and the entire query is in a single tdb file.

     This would also make the ~fuseki~ configuration more simple.


*** Exporting

    Here's a method to export data from your volume using the fuseki server.

    #+begin_src bash
      dc exec fuseki curl http://fuseki:3030/vocabularies/get -H "Accept:application/ld+json" | gzip > vocabularies.json.gz
    #+end_src
