#!/usr/bin/env bash
# These Global Variables are the defaults values for the current setup.
declare -g -A G=(
  [csv]="times.csv"
  # Below you probably don't need to change
  [host]="https://experts.ucdavis.edu"
  [shell_getopt]=${FLAGS_GETOPT_CMD:-getopt}
  [dry-run]=
  [dir]="."
  [jwt]=''
);

if ! opts=$(${G[shell_getopt]} -o h:j:c:d:n --long host:,jwt:,csv:,dir:,dry-run -n "count-experts" -- "$@"); then
  echo "Bad Command Options." >&2 ; exit 1 ; fi

eval set -- "$opts"

while true; do
  case "$1" in
    -h | --host ) G[host]="$2"; shift 2 ;;
    -j | --jwt ) G[jwt]="$2"; shift 2 ;;
    -c | --csv ) G[csv]="$2"; shift 2 ;;
    -n | --dry-run ) G[dry-run]=1; shift ;;
    -d | --dir ) G[dir]="$2"; shift 2 ;;
    -- ) shift; break ;;
    * ) break ;;
  esac
done

if [ -z "${G[jwt]}" ]; then
  echo "JWT is required. Use -j or --jwt to provide it." >&2 ; exit 1 ;
fi

mkdir -p "${G[dir]}/expert"

host=${G[host]}
# set session
http --session=dev ${host}/api/expert/browse/ Authorization:"Bearer ${G[jwt]}" >/dev/null 2>&1
http='http --session-read-only=dev'

echo "alpha,expert,time,status" > "${G[csv]}"

for i in A B C D E F G H I J K L M N O P Q R S T U V W X Y Z; do
  for e in $($http $host/api/expert/browse p==$i page==1 size==1000  | jq -r '.hits[]["@id"]'); do
    # Construct the URL
    url="$host/api/$e"
    # Use time and httpie to get timing and status code
    {
      start=$(date +%s.%N)
      #status=$($http --download --check-status "$url" -o $e.json 2>&1 | grep HTTP | awk '{print $2}')
      status=$($http --download --check-status -o $e.json GET "$url" all==1 include==hidden 2>&1 | grep HTTP | awk '{print $2}')
      end=$(date +%s.%N)
    } 2> /dev/null

    # Calculate elapsed time
    elapsed=$(echo "$end - $start" | bc)

    # Write to CSV
    echo "$i,$e,$elapsed,$status" | tee --append "${G[csv]}"
  done;
done

echo "Download times saved to $output"
